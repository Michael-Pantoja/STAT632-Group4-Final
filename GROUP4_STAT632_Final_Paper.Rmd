---
title: |  
  | \vspace{5cm} Predicting NBA Rookie Longevity Using Multiple Logistic Regression
  |
  | STAT 632
author: "GROUP 4: Michael Pantoja Collasso, Yuanyuan Fan, Wennie Wang"
output: pdf_document
urlcolor: blue
---
<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

\centering
\raggedright
\newpage

# I. Introduction

|     Over the past decade, the world of sports has erupted into a large economic powerhouse where millions of dollars are transferred  every day. A great example of this being the National Basketball Association (NBA) where players are now expected to receive millions of dollars worth of contracts every year. NBA teams typically like to reward better players with larger salaries as it makes sense to prioritize the better players and provides an incentive for all players to perform to the best of their abilities. Unfortunately, it is not always obvious to know which players are going to perform well and which players are going to be underwhelming. If a team were to somehow be able to accurately predict how well a player were to perform in the upcoming years, then it would be in that teams best interest to prioritize those players and try to keep them on their teams by providing satisfactory contracts. This exact though process is the reason we decided to take on this project.

|     Our goal is to try to create a model that will accurately decide if a player will still be playing in the league in 5 years. To do this we are going to run a multiple logistic model on numerous basketball related statistics to predict a players longevity. While basketball is mostly a numerical game, we decided to create a logistic model in an attempt to get a straightforward binary response -- will the player sill be playing in 5 years or not? This model is not meant to be a complex robust model that can predict outcomes, but rather, our goal is to create a model that would essentially act as the first step for someone who may be interested in doing further testing related to the data.



# II. Data Description

|     The data set that we used in this analysis comes from a [Data.World](https://data.world/exercises/logistic-regression-exercise-1) forum in which the statistics for every rookies first year in the league between 1992-2016 has been collected. This data set contains 1,329 observations with each observation having 21 different variables (Table 1). This has been truncated for formatting reasons but the full dictionary can be found in the aforementioned link.

| Variable Name | Meaning               | Variable Type |
|---------------|-----------------------|---------------|
| Name          | Player Name           | character           |
| GP            | Games Played          | numeric           |
| MIN           | Minutes Played        | numeric           |
| ...           | ...                   | ...               |
| BLK           | Blocks                | numeric           |
| TOV           | Turnovers             | numeric           |
| Target_5Yrs   | Career Length         | logical          |
Table: Data set dictionary 

|     Given that we plan on doing a logistic regression model, we are going to use the final variable, `Target_5Yrs` as our response variable. This is a Boolean in which a value of 1 means that the player is still playing in the NBA after 5 year while a value of 0 means that the player unfortunately is no longer playing in the NBA.

# III. Methods and Results

|     As mentioned before, our data set consists of 21 different variables. We exclude the target variable as well as the `name` variable as it would be nonsensical to assume that the name of a player will have an effect on the way they play. This leaves us with 19 numeric variables that we can use. Before creating a model, we can take a look at a correlation matrix to determine if there are any issues with collinearity (Figure 1).  

```{r, echo=FALSE,out.width="49%", out.height="20%",fig.cap="Correlation Matrix ",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("Col_1.png","Col_2.png", "Col_3.png", "Col_4.png"))
``` 

|     Taking a look at the correlation matrix for our variables we can see that there are values of all sorts. We can find high correlation coefficients such as the case with `REB` and `X3P.Made`. This signifies that there is a strong linear relationship between these variables and our response variable. However, there are also strong linear relationships between variables within the predictors themselves. This means that there are some collinearity issues that we need to deal wit. We can not just rely on the full model and we are going to have to do some pruning. However, before we did that we also decided to take a look at the leverage values to see if there are any data points that could cause issues (Figure 2). There do appear to be a few leverage points but we decided that these weren't too far from the rest of the clusters so we decided to keep them in our model.

<center>
<p>
![Leverage Points](Lev_1.png){width=45%}
</p>
</center>

\pagebreak

|     In an attempt to see which variables are signficant, we decided to use the backwards stepwise method to see if could prune some of the insignificant variables from our full model. In doing so, we are able to create a summer table that can show us which variables are significant (Table 2). 

| Coefficients  | Estimate  | Std. Error | Z Value | Pr(>\|z\|) |     |
|---------------|-----------|------------|---------|------------|-----|
| (Intercept)   | -4.221084 | 0.738821   | -5.713  | 1.11e-08   | *** |
| GP            | 0.03561   | 0.004674   | 7.544   | 4.56e-14   | *** |
| MIN           | -0.054651 | 0.031036   | -1.761  | 0.078256   | .   |
| FGA           | 0.093767  | 0.050893   | 1.842   | 0.065413   | .   |
| FG.           | 0.022131  | 0.013195   | 1.677   | 0.093490   | .   |
| X3P.Made      | 3.149994  | 1.017392   | 3.096   | 0.001961   | **  |
| X3PA          | -1.062457 | 0.382517   | -2.778  | 0.005477   | **  |
| FT.           | 0.015262  | 0.006794   | 2.246   | 0.024688   | *   |
| DREB          | -1.017435 | 0.304566   | -3.341  | 0.000836   | *** |
| REB           | 0.863982  | 0.219437   | 3.937   | 8.24e-05   | *** |
| AST           | 0.245440  | 0.085179   | 2.881   | 0.003958   | **  |
| BLK           | 0.557282  | 0.271478   | 2.053   | 0.040095   | *   |
Table: Summary Statistics 

|     Using the stepwise method, we were able to remove almost half of the variables that were present in the original model and we are now left with the following equation to represent our model: 

$log\frac{P(Target5Yrs)}{1-P(Target5yrs)} = 4.22 + 0.04GP - 0.05MIN + 0.09FGA + 0.02FG. + 3.15X3P.Made - 1.06X3PA + 0.02FT. - 1.02DREB + 0.86REB + 0.25AST + 0.56BLK$

|     It is important to mention that the model was created using only 70% of the original data set where each observation was chosen at random. The remaining 30% of the data will be used for testing our model. Using that testing data on our model, we were able to receive an accuracy of around 71.25%. We can take a better look at our results by creating a confusion matrix to compare the predicted values and the actual values (Table 3.) Given the fact that we are doing a logistic model, we provided a 0.5 probability threshold on the results. This means that all results less than 0.5 will be rounded to 0 and anything with a value of 0.5 or higher will be rounded to 1.

|    |  | Actual |  | |
|---------------|-----------|------------|---------|------------|
|               |           | 0          | 1       | Total      |
| Prediction    | 0         | 70   | 43   | 113   |
|               | 1         | 72   | 214  | 286   |
|               | Total     | 142   | 257   | 399   |
: Test Data Confusion Matrix

|     Just by looking at the confusion matrix, our model seems to be providing pretty solid results. Just looking at the `1` values, which again represent how many players will have careers longer than 5 years, our model does a pretty good job identifying those. A better way to determine how good our model performs is by taking a look at the ROC curve (Figure 3.)

|     Just by taking a quick glance at it, our model seems to be performing pretty well. An excellent model would have the solid line bunched up in the top left corner and a model that is no better than classifying values at random would be similar to the dashed line. Of course we can quantitatively determine how good our model is by taking the AUC, the area under the ROC curve. Doing so gives us a value of `0.7531`. To reiterate, a good model would have aan AUC of around 1 while a bad model would have a value close to 0.5. In general, an AUC value between 0.7 and 0.8 is considered good, it shows that our model is good, but we can try further optimization in the future.

<center>
<p>
![Model ROC Curve](ROC_1.png){width=75%}
</p>
</center>

# IV. Conclusion

|     Basketball is a popular sport all over the world, with a huge fan base where both fans and team owners are usually concerned about the career of their favorite players and hope that they can prolong their career to bring us more exciting games. Based on our model, if we want to obtain or predict relevant information, we can focus on the time and number of games played by players, also their Shooting, 3-point shooting, free throws and rebounding performance. And we were right about 72 percent of the time to support our predictions.

|     In the future, we can make more attempts on this subject, such as looking for other predictors to increase the accuracy of our model, and we can also try to use multiple linear regression models to analyze the coefficient of specific predictors to make further analysis and prediction. Also, it is imperative that we clean up the data set a little bit because it wasn't until after we completed our analysis that we noticed that there were some inconsistencies such as having repeated observations and some incorrect values for a few players. In that same process, we can also increase the size of the data set given the fact that these values ranged from 1992-2016. We can go ahead and take values up to this current year (2023) and perhaps get values from players from before 1992. Finally, it could be interesting to take a look at some more complex data. Instead of having a binary answer, it could be interesting to see just how many years our model predicts a player will be in the league for.

# V. Code Appendix

|     